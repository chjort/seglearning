Training with training/test data at:
  DATA_DIR: /mnt/data/nuclei-data
  MODEL_DIR: /job/model-code
  TRAINING_JOB: 
  TRAINING_COMMAND: python unet_nuclei.py --data_file nuclei_train_val.npz --epochs 200 --batch_size 16 --shape 256 --patience_stop 50
Storing trained model at:
  RESULT_DIR: /mnt/results/nuclei-result/training-Z1Yy68Eig
Fri Dec 14 11:27:35 UTC 2018: Running Tensorflow job
2018-12-14 11:27:43.958839: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-12-14 11:27:44.200832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:88:00.0
totalMemory: 11.92GiB freeMemory: 11.85GiB
2018-12-14 11:27:44.200881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:88:00.0, compute capability: 3.7)
Loading data... done
Building model... 
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 256, 256, 3)  0                                            
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, 256, 256, 3)  0           input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 256, 256, 16) 448         lambda_1[0][0]                   
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 256, 256, 16) 0           conv2d_1[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 256, 256, 16) 2320        dropout_1[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 16) 0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 128, 128, 32) 4640        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 128, 128, 32) 0           conv2d_3[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 128, 128, 32) 9248        dropout_2[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 32)   0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 64, 64, 64)   0           conv2d_5[0][0]                   
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 64, 64, 64)   36928       dropout_3[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 32, 32, 128)  0           conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 128)  147584      dropout_4[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 128)  0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 16, 16, 256)  295168      max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 16, 16, 256)  0           conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 16, 16, 256)  590080      dropout_5[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 128)  131200      conv2d_10[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 32, 32, 256)  0           conv2d_transpose_1[0][0]         
                                                                 conv2d_8[0][0]                   
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 32, 32, 128)  295040      concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 32, 32, 128)  0           conv2d_11[0][0]                  
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 32, 32, 128)  147584      dropout_6[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 64)   32832       conv2d_12[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 64, 64, 128)  0           conv2d_transpose_2[0][0]         
                                                                 conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 64, 64, 64)   73792       concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 64, 64, 64)   0           conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 64, 64, 64)   36928       dropout_7[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 32) 8224        conv2d_14[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 128, 128, 64) 0           conv2d_transpose_3[0][0]         
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 128, 128, 32) 18464       concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 128, 128, 32) 0           conv2d_15[0][0]                  
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 128, 128, 32) 9248        dropout_8[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_4 (Conv2DTrans (None, 256, 256, 16) 2064        conv2d_16[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 256, 256, 32) 0           conv2d_transpose_4[0][0]         
                                                                 conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 256, 256, 16) 4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 256, 256, 16) 0           conv2d_17[0][0]                  
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 256, 256, 16) 2320        dropout_9[0][0]                  
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 256, 256, 1)  17          conv2d_18[0][0]                  
==================================================================================================
Total params: 1,941,105
Trainable params: 1,941,105
Non-trainable params: 0
__________________________________________________________________________________________________
Training model... 
Train on 536 samples, validate on 134 samples
Epoch 1/200

Epoch 00001: val_dice_coef improved from -inf to 0.50389, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-01-0.5038930.h5
 - 23s - loss: 0.3854 - dice_coef: 0.2971 - val_loss: 0.2187 - val_dice_coef: 0.5039
Epoch 2/200

Epoch 00002: val_dice_coef improved from 0.50389 to 0.65336, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-02-0.6533630.h5
 - 23s - loss: 0.1675 - dice_coef: 0.6551 - val_loss: 0.1876 - val_dice_coef: 0.6534
Epoch 3/200

Epoch 00003: val_dice_coef improved from 0.65336 to 0.73797, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-03-0.7379692.h5
 - 19s - loss: 0.1337 - dice_coef: 0.7392 - val_loss: 0.1441 - val_dice_coef: 0.7380
Epoch 4/200

Epoch 00004: val_dice_coef improved from 0.73797 to 0.79318, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-04-0.7931796.h5
 - 19s - loss: 0.1158 - dice_coef: 0.7712 - val_loss: 0.1086 - val_dice_coef: 0.7932
Epoch 5/200

Epoch 00005: val_dice_coef improved from 0.79318 to 0.81429, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-05-0.8142880.h5
 - 18s - loss: 0.1009 - dice_coef: 0.8069 - val_loss: 0.1038 - val_dice_coef: 0.8143
Epoch 6/200

Epoch 00006: val_dice_coef did not improve
 - 15s - loss: 0.0930 - dice_coef: 0.8222 - val_loss: 0.1050 - val_dice_coef: 0.8111
Epoch 7/200

Epoch 00007: val_dice_coef did not improve
 - 15s - loss: 0.0910 - dice_coef: 0.8237 - val_loss: 0.1268 - val_dice_coef: 0.8027
Epoch 8/200

Epoch 00008: val_dice_coef improved from 0.81429 to 0.81805, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-08-0.8180466.h5
 - 18s - loss: 0.0915 - dice_coef: 0.8307 - val_loss: 0.0996 - val_dice_coef: 0.8180
Epoch 9/200

Epoch 00009: val_dice_coef improved from 0.81805 to 0.82466, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-09-0.8246600.h5
 - 18s - loss: 0.0836 - dice_coef: 0.8375 - val_loss: 0.1030 - val_dice_coef: 0.8247
Epoch 10/200

Epoch 00010: val_dice_coef improved from 0.82466 to 0.82851, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-10-0.8285104.h5
 - 18s - loss: 0.0823 - dice_coef: 0.8439 - val_loss: 0.0921 - val_dice_coef: 0.8285
Epoch 11/200

Epoch 00011: val_dice_coef did not improve
 - 15s - loss: 0.0831 - dice_coef: 0.8429 - val_loss: 0.0866 - val_dice_coef: 0.8273
Epoch 12/200

Epoch 00012: val_dice_coef improved from 0.82851 to 0.84109, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-12-0.8410859.h5
 - 18s - loss: 0.0777 - dice_coef: 0.8483 - val_loss: 0.0922 - val_dice_coef: 0.8411
Epoch 13/200

Epoch 00013: val_dice_coef improved from 0.84109 to 0.84941, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-13-0.8494062.h5
 - 18s - loss: 0.0760 - dice_coef: 0.8547 - val_loss: 0.0828 - val_dice_coef: 0.8494
Epoch 14/200

Epoch 00014: val_dice_coef did not improve
 - 15s - loss: 0.0731 - dice_coef: 0.8586 - val_loss: 0.0810 - val_dice_coef: 0.8486
Epoch 15/200

Epoch 00015: val_dice_coef did not improve
 - 15s - loss: 0.0742 - dice_coef: 0.8552 - val_loss: 0.0964 - val_dice_coef: 0.8354
Epoch 16/200

Epoch 00016: val_dice_coef improved from 0.84941 to 0.85157, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-16-0.8515719.h5
 - 18s - loss: 0.0741 - dice_coef: 0.8597 - val_loss: 0.0793 - val_dice_coef: 0.8516
Epoch 17/200

Epoch 00017: val_dice_coef improved from 0.85157 to 0.86409, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-17-0.8640928.h5
 - 18s - loss: 0.0703 - dice_coef: 0.8615 - val_loss: 0.0810 - val_dice_coef: 0.8641
Epoch 18/200

Epoch 00018: val_dice_coef did not improve
 - 15s - loss: 0.0712 - dice_coef: 0.8636 - val_loss: 0.0776 - val_dice_coef: 0.8471
Epoch 19/200

Epoch 00019: val_dice_coef did not improve
 - 15s - loss: 0.0693 - dice_coef: 0.8634 - val_loss: 0.0729 - val_dice_coef: 0.8588
Epoch 20/200

Epoch 00020: val_dice_coef improved from 0.86409 to 0.86541, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-20-0.8654066.h5
 - 18s - loss: 0.0683 - dice_coef: 0.8638 - val_loss: 0.0740 - val_dice_coef: 0.8654
Epoch 21/200

Epoch 00021: val_dice_coef did not improve
 - 15s - loss: 0.0665 - dice_coef: 0.8716 - val_loss: 0.0721 - val_dice_coef: 0.8638
Epoch 22/200

Epoch 00022: val_dice_coef did not improve
 - 15s - loss: 0.0680 - dice_coef: 0.8625 - val_loss: 0.0750 - val_dice_coef: 0.8601
Epoch 23/200

Epoch 00023: val_dice_coef did not improve
 - 15s - loss: 0.0662 - dice_coef: 0.8704 - val_loss: 0.0764 - val_dice_coef: 0.8575
Epoch 24/200

Epoch 00024: val_dice_coef did not improve
 - 15s - loss: 0.0670 - dice_coef: 0.8672 - val_loss: 0.0755 - val_dice_coef: 0.8636
Epoch 25/200

Epoch 00025: val_dice_coef did not improve
 - 15s - loss: 0.0662 - dice_coef: 0.8703 - val_loss: 0.0745 - val_dice_coef: 0.8555
Epoch 26/200

Epoch 00026: val_dice_coef did not improve

Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
 - 15s - loss: 0.0651 - dice_coef: 0.8732 - val_loss: 0.0710 - val_dice_coef: 0.8585
Epoch 27/200

Epoch 00027: val_dice_coef improved from 0.86541 to 0.86628, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-27-0.8662783.h5
 - 18s - loss: 0.0625 - dice_coef: 0.8734 - val_loss: 0.0714 - val_dice_coef: 0.8663
Epoch 28/200

Epoch 00028: val_dice_coef improved from 0.86628 to 0.86699, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-28-0.8669944.h5
 - 18s - loss: 0.0616 - dice_coef: 0.8743 - val_loss: 0.0702 - val_dice_coef: 0.8670
Epoch 29/200

Epoch 00029: val_dice_coef improved from 0.86699 to 0.86808, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-29-0.8680756.h5
 - 20s - loss: 0.0610 - dice_coef: 0.8779 - val_loss: 0.0717 - val_dice_coef: 0.8681
Epoch 30/200

Epoch 00030: val_dice_coef did not improve
 - 15s - loss: 0.0612 - dice_coef: 0.8781 - val_loss: 0.0694 - val_dice_coef: 0.8674
Epoch 31/200

Epoch 00031: val_dice_coef did not improve
 - 15s - loss: 0.0608 - dice_coef: 0.8774 - val_loss: 0.0713 - val_dice_coef: 0.8670
Epoch 32/200

Epoch 00032: val_dice_coef improved from 0.86808 to 0.86895, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-32-0.8689536.h5
 - 18s - loss: 0.0609 - dice_coef: 0.8786 - val_loss: 0.0712 - val_dice_coef: 0.8690
Epoch 33/200

Epoch 00033: val_dice_coef did not improve
 - 15s - loss: 0.0606 - dice_coef: 0.8803 - val_loss: 0.0709 - val_dice_coef: 0.8650
Epoch 34/200

Epoch 00034: val_dice_coef improved from 0.86895 to 0.87151, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-34-0.8715079.h5
 - 18s - loss: 0.0609 - dice_coef: 0.8782 - val_loss: 0.0692 - val_dice_coef: 0.8715
Epoch 35/200

Epoch 00035: val_dice_coef did not improve
 - 15s - loss: 0.0601 - dice_coef: 0.8807 - val_loss: 0.0721 - val_dice_coef: 0.8676
Epoch 36/200

Epoch 00036: val_dice_coef did not improve
 - 15s - loss: 0.0604 - dice_coef: 0.8800 - val_loss: 0.0703 - val_dice_coef: 0.8630
Epoch 37/200

Epoch 00037: val_dice_coef did not improve
 - 15s - loss: 0.0604 - dice_coef: 0.8807 - val_loss: 0.0705 - val_dice_coef: 0.8659
Epoch 38/200

Epoch 00038: val_dice_coef did not improve
 - 15s - loss: 0.0600 - dice_coef: 0.8802 - val_loss: 0.0694 - val_dice_coef: 0.8682
Epoch 39/200

Epoch 00039: val_dice_coef did not improve
 - 15s - loss: 0.0596 - dice_coef: 0.8797 - val_loss: 0.0707 - val_dice_coef: 0.8689
Epoch 40/200

Epoch 00040: val_dice_coef improved from 0.87151 to 0.87231, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-40-0.8723102.h5
 - 20s - loss: 0.0593 - dice_coef: 0.8816 - val_loss: 0.0695 - val_dice_coef: 0.8723
Epoch 41/200

Epoch 00041: val_dice_coef did not improve
 - 15s - loss: 0.0594 - dice_coef: 0.8819 - val_loss: 0.0707 - val_dice_coef: 0.8671
Epoch 42/200

Epoch 00042: val_dice_coef did not improve
 - 15s - loss: 0.0590 - dice_coef: 0.8800 - val_loss: 0.0690 - val_dice_coef: 0.8711
Epoch 43/200

Epoch 00043: val_dice_coef did not improve
 - 15s - loss: 0.0589 - dice_coef: 0.8832 - val_loss: 0.0716 - val_dice_coef: 0.8701
Epoch 44/200

Epoch 00044: val_dice_coef did not improve
 - 15s - loss: 0.0591 - dice_coef: 0.8807 - val_loss: 0.0700 - val_dice_coef: 0.8670
Epoch 45/200

Epoch 00045: val_dice_coef did not improve
 - 15s - loss: 0.0591 - dice_coef: 0.8809 - val_loss: 0.0684 - val_dice_coef: 0.8703
Epoch 46/200

Epoch 00046: val_dice_coef did not improve

Epoch 00046: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
 - 15s - loss: 0.0584 - dice_coef: 0.8833 - val_loss: 0.0692 - val_dice_coef: 0.8678
Epoch 47/200

Epoch 00047: val_dice_coef did not improve
 - 15s - loss: 0.0577 - dice_coef: 0.8819 - val_loss: 0.0704 - val_dice_coef: 0.8690
Epoch 48/200

Epoch 00048: val_dice_coef did not improve
 - 15s - loss: 0.0571 - dice_coef: 0.8841 - val_loss: 0.0705 - val_dice_coef: 0.8705
Epoch 49/200

Epoch 00049: val_dice_coef did not improve
 - 15s - loss: 0.0571 - dice_coef: 0.8866 - val_loss: 0.0703 - val_dice_coef: 0.8717
Epoch 50/200

Epoch 00050: val_dice_coef did not improve
 - 15s - loss: 0.0571 - dice_coef: 0.8848 - val_loss: 0.0717 - val_dice_coef: 0.8691
Epoch 51/200

Epoch 00051: val_dice_coef improved from 0.87231 to 0.87247, saving model to /mnt/results/nuclei-result/_wml_checkpoints/model-51-0.8724676.h5
 - 19s - loss: 0.0570 - dice_coef: 0.8858 - val_loss: 0.0693 - val_dice_coef: 0.8725
Epoch 52/200

Epoch 00052: val_dice_coef did not improve
 - 15s - loss: 0.0571 - dice_coef: 0.8856 - val_loss: 0.0688 - val_dice_coef: 0.8724
Epoch 53/200

Epoch 00053: val_dice_coef did not improve
 - 15s - loss: 0.0569 - dice_coef: 0.8851 - val_loss: 0.0706 - val_dice_coef: 0.8710
Epoch 54/200

Epoch 00054: val_dice_coef did not improve
 - 15s - loss: 0.0568 - dice_coef: 0.8851 - val_loss: 0.0705 - val_dice_coef: 0.8717
Epoch 55/200

Epoch 00055: val_dice_coef did not improve
 - 15s - loss: 0.0567 - dice_coef: 0.8869 - val_loss: 0.0706 - val_dice_coef: 0.8713
Epoch 56/200

Epoch 00056: val_dice_coef did not improve
 - 15s - loss: 0.0566 - dice_coef: 0.8861 - val_loss: 0.0709 - val_dice_coef: 0.8712
Epoch 57/200

Epoch 00057: val_dice_coef did not improve

Epoch 00057: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
 - 15s - loss: 0.0565 - dice_coef: 0.8883 - val_loss: 0.0710 - val_dice_coef: 0.8724
Epoch 58/200

Epoch 00058: val_dice_coef did not improve
 - 15s - loss: 0.0562 - dice_coef: 0.8872 - val_loss: 0.0708 - val_dice_coef: 0.8721
Epoch 59/200

Epoch 00059: val_dice_coef did not improve
 - 15s - loss: 0.0562 - dice_coef: 0.8882 - val_loss: 0.0710 - val_dice_coef: 0.8720
Epoch 60/200

Epoch 00060: val_dice_coef did not improve
 - 15s - loss: 0.0562 - dice_coef: 0.8863 - val_loss: 0.0706 - val_dice_coef: 0.8719
Epoch 61/200

Epoch 00061: val_dice_coef did not improve
 - 15s - loss: 0.0562 - dice_coef: 0.8869 - val_loss: 0.0710 - val_dice_coef: 0.8717
Epoch 62/200

Epoch 00062: val_dice_coef did not improve

Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
 - 15s - loss: 0.0562 - dice_coef: 0.8873 - val_loss: 0.0706 - val_dice_coef: 0.8720
Epoch 63/200

Epoch 00063: val_dice_coef did not improve
 - 15s - loss: 0.0562 - dice_coef: 0.8868 - val_loss: 0.0708 - val_dice_coef: 0.8720
Epoch 64/200

Epoch 00064: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8876 - val_loss: 0.0708 - val_dice_coef: 0.8719
Epoch 65/200

Epoch 00065: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8869 - val_loss: 0.0709 - val_dice_coef: 0.8718
Epoch 66/200

Epoch 00066: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8875 - val_loss: 0.0709 - val_dice_coef: 0.8718
Epoch 67/200

Epoch 00067: val_dice_coef did not improve

Epoch 00067: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
 - 15s - loss: 0.0561 - dice_coef: 0.8873 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 68/200

Epoch 00068: val_dice_coef did not improve
 - 15s - loss: 0.0562 - dice_coef: 0.8858 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 69/200

Epoch 00069: val_dice_coef did not improve
 - 15s - loss: 0.0560 - dice_coef: 0.8864 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 70/200

Epoch 00070: val_dice_coef did not improve
 - 15s - loss: 0.0560 - dice_coef: 0.8879 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 71/200

Epoch 00071: val_dice_coef did not improve
 - 15s - loss: 0.0559 - dice_coef: 0.8873 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 72/200

Epoch 00072: val_dice_coef did not improve

Epoch 00072: ReduceLROnPlateau reducing learning rate to 1e-07.
 - 15s - loss: 0.0561 - dice_coef: 0.8872 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 73/200

Epoch 00073: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8874 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 74/200

Epoch 00074: val_dice_coef did not improve
 - 15s - loss: 0.0560 - dice_coef: 0.8880 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 75/200

Epoch 00075: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8873 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 76/200

Epoch 00076: val_dice_coef did not improve
 - 15s - loss: 0.0560 - dice_coef: 0.8870 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 77/200

Epoch 00077: val_dice_coef did not improve

Epoch 00077: ReduceLROnPlateau reducing learning rate to 1e-07.
 - 15s - loss: 0.0561 - dice_coef: 0.8875 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 78/200

Epoch 00078: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8879 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 79/200

Epoch 00079: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8868 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 80/200

Epoch 00080: val_dice_coef did not improve
 - 15s - loss: 0.0562 - dice_coef: 0.8863 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 81/200

Epoch 00081: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8865 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 82/200

Epoch 00082: val_dice_coef did not improve

Epoch 00082: ReduceLROnPlateau reducing learning rate to 1e-07.
 - 15s - loss: 0.0560 - dice_coef: 0.8874 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 83/200

Epoch 00083: val_dice_coef did not improve
 - 15s - loss: 0.0560 - dice_coef: 0.8870 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 84/200

Epoch 00084: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8876 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 85/200

Epoch 00085: val_dice_coef did not improve
 - 15s - loss: 0.0560 - dice_coef: 0.8872 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 86/200

Epoch 00086: val_dice_coef did not improve
 - 15s - loss: 0.0560 - dice_coef: 0.8861 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 87/200

Epoch 00087: val_dice_coef did not improve

Epoch 00087: ReduceLROnPlateau reducing learning rate to 1e-07.
 - 15s - loss: 0.0560 - dice_coef: 0.8875 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 88/200

Epoch 00088: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8850 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 89/200

Epoch 00089: val_dice_coef did not improve
 - 15s - loss: 0.0560 - dice_coef: 0.8876 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 90/200

Epoch 00090: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8877 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 91/200

Epoch 00091: val_dice_coef did not improve
 - 15s - loss: 0.0560 - dice_coef: 0.8880 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 92/200

Epoch 00092: val_dice_coef did not improve

Epoch 00092: ReduceLROnPlateau reducing learning rate to 1e-07.
 - 15s - loss: 0.0561 - dice_coef: 0.8855 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 93/200

Epoch 00093: val_dice_coef did not improve
 - 15s - loss: 0.0560 - dice_coef: 0.8866 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 94/200

Epoch 00094: val_dice_coef did not improve
 - 15s - loss: 0.0560 - dice_coef: 0.8866 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 95/200

Epoch 00095: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8866 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 96/200

Epoch 00096: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8875 - val_loss: 0.0709 - val_dice_coef: 0.8719
Epoch 97/200

Epoch 00097: val_dice_coef did not improve

Epoch 00097: ReduceLROnPlateau reducing learning rate to 1e-07.
 - 15s - loss: 0.0561 - dice_coef: 0.8870 - val_loss: 0.0710 - val_dice_coef: 0.8719
Epoch 98/200

Epoch 00098: val_dice_coef did not improve
 - 15s - loss: 0.0560 - dice_coef: 0.8875 - val_loss: 0.0710 - val_dice_coef: 0.8719
Epoch 99/200

Epoch 00099: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8875 - val_loss: 0.0710 - val_dice_coef: 0.8719
Epoch 100/200

Epoch 00100: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8868 - val_loss: 0.0710 - val_dice_coef: 0.8719
Epoch 101/200

Epoch 00101: val_dice_coef did not improve
 - 15s - loss: 0.0561 - dice_coef: 0.8862 - val_loss: 0.0710 - val_dice_coef: 0.8719
/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Elapsed training time: 1576.266191959381
Saving model... done
